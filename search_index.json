[
["index.html", "RAP Companion Preface 0.1 Discovery 0.2 Getting the balance right 0.3 Inspiration", " RAP Companion Matthew Gregory, Matthew Upson 2017-10-10 Preface Producing official statistics for publications is a key function of many teams across Government. It’s a time consuming and meticulous process to ensure that statistics are accurate and timely. With open source software becoming more widely used, there’s now a range of tools and techniques that can be used to reduce production time, whilst maintaining and even improving the quality of the publications. This book is about these techniques: what they are, and how we can use them. 0.1 Discovery Something is better than nothing. This is NOT TRUE in the agile world; consider the opportunity cost1. Prior to commiting to RAP development in your team ask yourself two questions: Is there a genuine user need for this new product/service? Does another product/service exist that you can re-use or adapt?2 0.2 Getting the balance right Whilst incredibly powerful, these approaches should not be seen as panacea for all the difficulties of statistics production: however, implementing even a few of these techniques can drive benefits in auditability, speed, quality, and knowledge transfer. There is a balance to be struck between ease of maintenance and the level of automation: this is likely to differ for every publication or team. These techniques are however tried and tested for software development and most already feature in the Service Manual: in this project we have just applied these methodologies to a new area. 0.3 Inspiration This book was inspired by, and draws heavily from a Government Digital Service blog post by Dr. Matthew Upson, itself taking much inspiration from the fields of DevOps and reproducible research. the loss of other alternatives when one alternative is chosen↩ There’s probably some code you can adapt on Github, such as the eesectors pacakge↩ "],
["intro.html", "Chapter 1 Introduction 1.1 Objectives", " Chapter 1 Introduction The Reproducible Analytical Pipeline (RAP) is an alternative production methodology for automating the bulk of steps involved in creating a statistical report. The production of a RAP is not a trivial task given technical, time and knowledge limitations within teams who desire to replace traditional and semi-manual methods of statistical report production. We address some of the common knowledge gaps and hard-to-Google problems that upcoming RAP-pers face. This bookdown manual uses the eesectors package to help elucidate the RAP development process. It attempts to almagamate the learnings of the early pioneers of RAP into a maintained tome of knowledge to ensure good practice is accessible for RAP newbies. 1.1 Objectives Questions to ask oneself before you RAP? Why RAP? How to RAP? "],
["ethics.html", "Chapter 2 Ethical Considerations 2.1 Extra detail", " Chapter 2 Ethical Considerations The RAP is considered Data Science in that it involves an overlap of Computer Science, Statistics and Expert Domain Knowledge. Data science ethics are important. We consult the ethical framework before proceeding. We use the recommended Six Key Principles to structure our thinking: Start with clear user need and public benefit Use data and tools which have the minimum intrusion necessary Create robust data science models Be alert to public perceptions Be as open and accountable as possible Keep data secure Fundamentally, the public benefit of doing the project needs to be balanced against the risks of doing so. 2.1 Extra detail Working through the quick checklist in the Ethical Framework we can identify those Principles that are more pertinent to a RAP project. We discuss some of these considerations in more detail here and the questions you should ask your team and stakeholders before proceeding with development. 2.1.1 Start with clear user need and public benefit How does the department and public benefit? Is this a vanity project? How many statistical reports does your team produce? What proportion of their time does this take up? How much copy and pasting (data movement) between softwares is involved? Have you demonstrated the benefit to your team? (Show the thing.) RAP has reduced the time taken to produce a report from two months to two weeks. - placeholder 2.1.2 Use data and tools which have the minimum intrusion necessary How intrusive and identifiable is the data you are working with? What is the minimum data necessary to achieve the user benefit? Do you really need to use sensitive data (de-identify or aggregate it)? Have you made synthetic data, that looks like real data, for the RAP software development? If identifying individuals, how widely are you searching personal data? 2.1.3 Create robust data science models What is the quality of the data? How representative is the data? (what are the biases?) Are there any automated decisions? What safeguards can be implemented to validate any automatic decision making? What is the risk that someone will suffer a negative unintended consequence as a result of the project? What are all the data checking and validation steps that occur (if you can explain it to a human you should be able to code it)? 2.1.4 Be alert to public perceptions If personal data for operational purposes, how compatible was it with the reason collected? Do the public agree with what you are doing? Automation often has negative conotations - why is RAP useful to everyone involved? 2.1.5 Be as open and accountable as possible How open can you be about the project? How much oversight and accountability is there throughout the project? Can you explain in plain English why you are developing a RAP? Can you get colleagues from other departments to review your Pull Requests? 2.1.6 Keep data secure How secure is your data? You will be using a version control software like Git. How will you prevent your RAP developers from accidentally pushing sensitive data to Github? Are you following GDS best practice? Have you considered using tools, like Git hooks, to prevent the unintended publication of sensitive data on Github? "],
["why.html", "Chapter 3 Why RAP? 3.1 The current statistics production process 3.2 Why learning to RAP might be hard", " Chapter 3 Why RAP? Reproducible Analytical Pipelines require a range of tools and techniques to implement that can be a challenge to overcome; so why bother learning to RAP? Does your team spend too much time moving data between various softwares? Could you reproduce your most recent publication’s stats? How about from five years ago? What would your team do with their time if it was freed up from copying and pasting? What proportion of spreadsheets contain errors? 3.1 The current statistics production process Here we use the production of official statistics in Government; altough this process varies widely perhaps it looks something like this? Broadly speaking, data are extracted from a datastore (whether it is a data lake, database, spreadsheet, or flat file), and are manipulated in proprietary statistical software, and possibly in proprietary spreadsheet software. Formatted tables are often then ‘copy and pasted’ into a word processor, before being converted to pdf format, and finally published to GOV.UK. This is quite a simplification, as statistical publications are usually produced by several people, so this process is likely to be happening in parallel many times. Often quality assurance (QA) is a process takes place at the end of the process. At its simplest, this may be a word processor document or spreadhseet sent by email to colleagues for review, prior to being approved for publication. 3.1.1 The problem with this approach Errors in spreadsheets are common due to human error3. We can mitigate these issues by minimising the role humans play in the tasks that they perform poorly4. This will free up human time to focus on complex tasks such as the interpretation of the statistics and communicating the implications of these findings to others. Other issues are: Copy and pasting. Lack of proper version control. Testing and QA often happens at the end, not throughout. A key element in this process is quality assurance. Each publication is meticulously checked to ensure the accuracy of the statistics being produced. This may take place throughout the production process or at the end prior to publication. Traditionally, QA has been a manual process which can take up a significant portion of the overall production time of a publication, as any changes will require the manual process of production to be repeated. 3.1.2 Desired Reproducible Analytical Pipeline An analytical pipeline should have all the hallmarks of good scientific practice - it should be reproducible. We should make use of modern DataOps principles, tools and techniques to improve the quality of the final product in a more timely fashion compared to the traditional method. All these parallel work flows to produce different parts of the report (e.g. tables and figures represented by different colours) can be conducted by bespoke functions within an R software package (we are language agnostic but prefered R and Rmarkdown for our DCMS work due to their prior experience with R). Rather than copying and pasting between different software, we read our data in once using a tidy data set that we create. We then do all the analysis and report production within R and benefit from many open source tools that make up RAP. These are represented by the logos along the bottom; git, Github, Travis, Appveyor and codecov. 3.2 Why learning to RAP might be hard Automating statistical report production is not without its challenges. RAP requires changes to your processes and is dependent on tools and techniques without which your journey may fail. Based on ours and others’ experiences, here are some things that will make your road to RAP difficult (though not impossible): Data format changes unexpectedly: e.g. the source data are not stored in a relational database, but are provided from a third party in spreadsheets. Publication format changes frequently. Limited access to open source tools; ideally you would need access to the following: Rstudio R Rtools (required to build packages) git (command line or GUI - this is extremely important) github (need to be able to push and pull to/from a remote repository) Not coding in the open. Not coding in the open makes it much harder to collaborate, as you will not have free access to a range of automated tools like travis/appveyor/codecov/coveralls which are part of the RAP model. Business as usual (BAU): It’s not possible to do this kind of work whilst also doing BAU tasks. It takes time to get to grips with the techniques, and the first version should be considered a prototype that needs to be at least partly validated against a manual procedure. Research on spreadsheet errors is substantial, compelling, and unanimous. It has three simple conclusions. The first is that spreadsheet errors are rare on a per-cell basis, but in large programs, at least one incorrect bottom-line value is very likely to be present. The second is that errors are extremely difficult to detect and correct. The third is that spreadsheet developers and corporations are highly overconfident in the accuracy of their spreadsheets.↩ Tasks that machines excel at↩ "],
["exemplar.html", "Chapter 4 Exemplar RAP 4.1 Package Purpose 4.2 Tidy data 4.3 eesectors Package Exploration 4.4 Chapter plenary", " Chapter 4 Exemplar RAP Chapter 3 considered why RAP is a useful paradigm. In this Chapter we demonstrate a RAP package developed in collaboration with the Department for Culture Media and Sport (DCMS). This package enshrines all the pertinent business knowledge in one corpus. 4.1 Package Purpose In this exemplar project Matt Upson aimed at a high level of automation to demonstrate what is possible, and because DCMS had a skilled data scientist on hand to maintain and develop the project. Nonetheless, in the course of the work, statisticians at DCMS continue to undertake training in R, and the Better Use of Data Team spent time to ensure that the software development practices such as managing software dependencies, version control, package development, unit testing, style guide, open by default and continuous integration are embedded within the team that owns the publication. We’re continuing to support DCMS in the development of this prototype pipeline, with the expectation that it will be used operationally in 2017. If you want to learn more about this project, the source code for the eesectors R package is maintained on GitHub.com. The README provides instructions on how to test the package using the openly published data from the 2016 publication. 4.2 Tidy data Tidy data are all alike; every messy data is messy in its own way. - Hadley Tolstoy What is the simplest representation of the data possible? Prior to any analysis we must tidy our data: structuring our data to facilitate analysis. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. You and your team trying to RAP should spend time reading this paper and hold a seminar discussing it. It’s important to involve the analysts involved in the traditional production of this report as they will be familiar with the inputs and outputs of the report. With the heuristic of a tidy dataset in your mind, proceed, as a team, to look through the chapter or report you are attempting to produce using RAP. As you work through, note down what variables you would need to produce each table or figure, what would the input dataframe look like? (Say what you see.) After looking at all the figures and tables, is there one tidy daaset that could be used as input? Sketch out what it looks like. 4.2.1 eesectors tidy data We demonstrate this process using the DCMS publication, refer to Chapter 3 - GVA. What data do you need to produce this table? Variables: Year, Sector, GVA What data do you need to produce this figure? The GVA of each Sector by Year. Variables: Year, Sector, GVA What data do you need to produce this figure? Total GVA across all sectors. Variables: Year, Sector, GVA What data do you need to produce this figure? For each Year by Sector we need the GVA. Variables: Year, Sector, GVA 4.2.2 What does our eesectors tidy data look like? Remember, for tidy data: 1. Each variable forms a column. 2. Each observation forms a row. 3. Each type of observational unit forms a table. Our tidy data is of the form Year - Sector - GVA: Year Sector GVA 2010 creative 65188 2010 culture 20291 2010 digital 97303 2011 creative 69398 2011 culture 20954 2011 digital 107303 This data is for demonstration purposes only. 4.2.3 How to build your tidy data? With the minimal tidy dataset idea in place, you can begin to think about how you might construct this tidy dataset from the data stores you have availiable. As we are working in R we can formalise this minimal tidy dataset as a class. For our eesectors package we create our long data year_sector_data class as the fundamental input to create all our figures and tables for the output report. 4.3 eesectors Package Exploration The following is an exploration of the eesectors package to help familiarise users with the key principles so that they can automate report production through package development in R using knitr. This examines the package in more detail compared to the README so that data scientists looking to implement RAP can note some of the characteristics of the code employed. 4.3.1 Installation The package can then be installed using devtools::install_github('ukgovdatascience/eesectors'). Some users may not be able to use the devtools::install_github() commands as a result of network security settings. If this is the case, eesectors can be installed by downloading the zip of the repository and installing the package locally using devtools::install_local(&lt;path to zip file&gt;). 4.3.1.1 Version control As the code is stored on Github we can access the current master version as well as all historic versions. This allows me to reproduce a report from last year if required. I can look at what release version was used and install that accordingly using the additional arguments for install_github. 4.3.2 Loading the package Installation means the package is on our computer but it is not loaded into the computer’s working memory. We also load any additional packages that might be useful for exploring the package or data therein. ## Using GitHub PAT from envvar GITHUB_PAT ## Skipping install of &#39;eesectors&#39; from a github remote, the SHA1 (5e91f359) has not changed since last install. ## Use `force = TRUE` to force installation ## eesectors: Reproducible Analytical Pipeline (RAP) for the ## Economic Estimates for DCMS Sectors Statistical First Release ## (SFR). For more information visit: ## https://github.com/ukgovdatascience/eesectors This makes all the functions within the package available for use. It also provides us with some R data objects, such as aggregated data sets ready for visualisations or analysis within the report. Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. - Hadely Wickham 4.3.3 Explore the package A good place to start is the package README. 4.3.3.1 Status badges The status badges provide useful information. They are found in the top left of the README and should be green and say passing. This indicates that this package will run OK on Windows and linux or mac. Essentially the package is likely to build correctly on your machine when you install it. You can carry out these build tests locally using the devtools package. 4.3.3.2 Look at the output first If you go to Chapter 3 of the DCMS publication it is apparent that most of the content is either data tables of summary statistics or visualisation of the data. This makes automation particularly useful here and likely to make time savings. Chapter 3 seems to be fairly typical in its length (if not a bit shroter compared to other Chapters). This package seems to work by taking the necessary data inputs as arguments in a function then outputting the relevant figures. The names of the functions match the figures they produce. Prior to this step we have to get the data in the correct format. If you look at the functions within the package within R Studio using the package navigator it is evident that there are a function of families dedicated to reading Excel spreadsheets and collecting the data in a tidy .Rds format. These are given the funciton name-prefix of extract_ (try to give your functions good names). The GVA_by_sector_2016 provides test data to work with during development. This will be important for the development of other packages for different reports. You need a precise understanding of how you go from raw data, to aggregated data (such as GVA_by_sector_2016) to the final figure. What are your inputs (arguments) and outputs? In some cases where your master data is stored in a particularly difficult for a machine to read you may prefer having a human to this extraction step. dplyr::glimpse(GVA_by_sector_2016) ## Observations: 54 ## Variables: 3 ## $ sector &lt;fctr&gt; creative, culture, digital, gambling, sport, telecoms,... ## $ year &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2... ## $ GVA &lt;dbl&gt; 65188, 20291, 97303, 8407, 7016, 24738, 49150, 69398, 2... x &lt;- GVA_by_sector_2016 4.3.3.3 Automating QA Human’s are not particularly good at Quality Assurance (QA), especially when working with massive spreadsheets it’s easy for errors to creep in. We can automate alot of the sense checking and update this if things change or a human provides another creative test to use for sense checking. If you can describe the test to a colleague then you can code it. The author uses messages to tell us what checks are being conducted or we can look at the body of the function if we are interested. This is useful if you are considering developing your own package, it will help you struture the message which are useful for the user. gva &lt;- year_sector_data(GVA_by_sector_2016) ## Initiating year_sector_data class. ## ## ## Expects a data.frame with three columns: sector, year, and measure, where ## measure is one of GVA, exports, or enterprises. The data.frame should include ## historical data, which is used for checks on the quality of this year&#39;s data, ## and for producing tables and plots. More information on the format expected by ## this class is given by ?year_sector_data(). ## ## *** Running integrity checks on input dataframe (x): ## ## Checking input is properly formatted... ## Checking x is a data.frame... ## Checking x has correct columns... ## Checking x contains a year column... ## Checking x contains a sector column... ## Checking x does not contain missing values... ## Checking for the correct number of rows... ## ...passed ## ## ***Running statistical checks on input dataframe (x)... ## ## These tests are implemented using the package assertr see: ## https://cran.r-project.org/web/packages/assertr for more details. ## Checking years in a sensible range (2000:2020)... ## Checking sectors are correct... ## Checking for outliers (x_i &gt; median(x) + 3 * mad(x)) in each sector timeseries... ## Checking sector timeseries: all_dcms ## Checking sector timeseries: creative ## Checking sector timeseries: culture ## Checking sector timeseries: digital ## Checking sector timeseries: gambling ## Checking sector timeseries: sport ## Checking sector timeseries: telecoms ## Checking sector timeseries: tourism ## Checking sector timeseries: UK ## ...passed ## Checking for outliers on a row by row basis using mahalanobis distance... ## Checking sector timeseries: all_dcms ## Checking sector timeseries: creative ## Checking sector timeseries: culture ## Checking sector timeseries: digital ## Checking sector timeseries: gambling ## Checking sector timeseries: sport ## Checking sector timeseries: telecoms ## Checking sector timeseries: tourism ## Checking sector timeseries: UK ## ...passed This is a semi-automated process so the user should check the Checks and ensure they meet their usual checks that would be conducted manually. If a new check or test becomes necessary then it should be implemented by changing the code. body(year_sector_data) ## { ## message(&quot;Initiating year_sector_data class.\\\\n\\\\n\\\\nExpects a data.frame with three columns: sector, year, and measure, where\\\\nmeasure is one of GVA, exports, or enterprises. The data.frame should include\\\\nhistorical data, which is used for checks on the quality of this year&#39;s data,\\\\nand for producing tables and plots. More information on the format expected by\\\\nthis class is given by ?year_sector_data().&quot;) ## message(&quot;\\\\n*** Running integrity checks on input dataframe (x):&quot;) ## message(&quot;\\\\nChecking input is properly formatted...&quot;) ## message(&quot;Checking x is a data.frame...&quot;) ## if (!is.data.frame(x)) ## stop(&quot;x must be a data.frame&quot;) ## message(&quot;Checking x has correct columns...&quot;) ## if (length(colnames(x)) != 3) ## stop(&quot;x must have three columns: sector, year, and one of GVA, export, or x&quot;) ## message(&quot;Checking x contains a year column...&quot;) ## if (!&quot;year&quot; %in% colnames(x)) ## stop(&quot;x must contain year column&quot;) ## message(&quot;Checking x contains a sector column...&quot;) ## if (!&quot;sector&quot; %in% colnames(x)) ## stop(&quot;x must contain sector column&quot;) ## message(&quot;Checking x does not contain missing values...&quot;) ## if (anyNA(x)) ## stop(&quot;x cannot contain any missing values&quot;) ## message(&quot;Checking for the correct number of rows...&quot;) ## if (nrow(x) != length(unique(x$sector)) * length(unique(x$year))) { ## warning(&quot;x does not appear to be well formed. nrow(x) should equal\\\\nlength(unique(x$sector)) * length(unique(x$year)). Check the of x.&quot;) ## } ## message(&quot;...passed&quot;) ## message(&quot;\\\\n***Running statistical checks on input dataframe (x)...\\\\n\\\\n These tests are implemented using the package assertr see:\\\\n https://cran.r-project.org/web/packages/assertr for more details.&quot;) ## value &lt;- colnames(x)[(!colnames(x) %in% c(&quot;sector&quot;, &quot;year&quot;))] ## message(&quot;Checking years in a sensible range (2000:2020)...&quot;) ## assertr::assert_(x, assertr::in_set(2000:2020), ~year) ## message(&quot;Checking sectors are correct...&quot;) ## sectors_set &lt;- c(creative = &quot;Creative Industries&quot;, culture = &quot;Cultural Sector&quot;, ## digital = &quot;Digital Sector&quot;, gambling = &quot;Gambling&quot;, sport = &quot;Sport&quot;, ## telecoms = &quot;Telecoms&quot;, tourism = &quot;Tourism&quot;, all_dcms = &quot;All DCMS sectors&quot;, ## perc_of_UK = &quot;% of UK GVA&quot;, UK = &quot;UK&quot;) ## assertr::assert_(x, assertr::in_set(names(sectors_set)), ## ~sector, error_fun = raise_issue) ## message(&quot;Checking for outliers (x_i &gt; median(x) + 3 * mad(x)) in each sector timeseries...&quot;) ## series_split &lt;- split(x, x$sector) ## lapply(X = series_split, FUN = function(x) { ## message(&quot;Checking sector timeseries: &quot;, unique(x[[&quot;sector&quot;]])) ## assertr::insist_(x, assertr::within_n_mads(3), lazyeval::interp(~value, ## value = as.name(value)), error_fun = raise_issue) ## }) ## message(&quot;...passed&quot;) ## message(&quot;Checking for outliers on a row by row basis using mahalanobis distance...&quot;) ## lapply(X = series_split, FUN = maha_check) ## message(&quot;...passed&quot;) ## structure(list(df = x, colnames = colnames(x), type = colnames(x)[!colnames(x) %in% ## c(&quot;year&quot;, &quot;sector&quot;)], sector_levels = levels(x$sector), ## sectors_set = sectors_set, years = unique(x$year)), class = &quot;year_sector_data&quot;) ## } The function is structured to tell the user what check is being made and then running that check given the input x. If the input fails a check the function is stopped with a useful diagnostic message for the user. This is achieved using if and the opposite of the desired feature of x. message(&quot;Checking x has correct columns...&quot;) if (length(colnames(x)) != 3) stop(&quot;x must have three columns: sector, year, and one of GVA, export, or x&quot;) For example, if x does not have exactly three columns we stop. 4.3.3.4 Output of this function The output object is different to the input as expected, yet it does contain the initial data. identical(gva$df, x) ## [1] TRUE The rest of the list contains other details that could be changed at a later date if required, demonstrating defensive programming. For example, the sectors that are of interest to DCMS have changed and may change again. ?year_sector_data Let’s take a closer look at this function using the help and other standard function exploration functions. The help says it produces a custom class of object with five slots. isS4(gva) ## [1] FALSE class(gva) ## [1] &quot;year_sector_data&quot; It’s not actually an S4 object, by slots the author means a list of objects. This approach is sensible and easy to work with, as most users are familiar with S3. 4.3.3.5 The input The input, which is likely a bunch of not tidy or messy spreadsheets needs to be wrangled and aggregated (if necessary) for input into the functions prefixed by figure. dplyr::glimpse(GVA_by_sector_2016) ## Observations: 54 ## Variables: 3 ## $ sector &lt;fctr&gt; creative, culture, digital, gambling, sport, telecoms,... ## $ year &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2... ## $ GVA &lt;dbl&gt; 65188, 20291, 97303, 8407, 7016, 24738, 49150, 69398, 2... 4.3.3.6 The R output We build our functions to use the same simple, tidy, data. - Matt Upson With the data in the appropriate form to be received as an argument or input for the figure family of functions, we can proceed to plot. figure3.1(x = gva) Again we can look at the details of the plot. We could change the body of the function to affect change to the default plot or we can pass additional ggplot arguments to it. Reading the code we see it filters the data, makes the variables it needs, refactors the sector variable and then plots it. body(figure3.1) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~sector != &quot;UK&quot;) ## x &lt;- dplyr::mutate_(x, year = ~factor(year, levels = c(2016:2010))) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)]), ## levels = rev(as.character(unname(sectors_set[levels(x$sector)])))) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~GVA, x = ~sector, ## fill = ~year) + ggplot2::geom_bar(colour = &quot;slategray&quot;, ## position = &quot;dodge&quot;, stat = &quot;identity&quot;) + ggplot2::coord_flip() + ## govstyle::theme_gov(base_colour = &quot;black&quot;) + ggplot2::scale_fill_brewer(palette = &quot;Blues&quot;) + ## ggplot2::ylab(&quot;Gross Value Added (£bn)&quot;) + ggplot2::theme(legend.position = &quot;right&quot;, ## legend.key = ggplot2::element_blank()) + ggplot2::scale_y_continuous(labels = scales::comma) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.1():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.1():&quot;, e) ## }, finally = { ## }) ## } We can inspect and change an argument if we feel inclined or if a new colour scheme becomes preferred for example. However, there is no ... in the body of the function itself so where does this argument get passed to? This all looks straight forward and we can inspect the other functions for generating the figures or plot output. body(figure3.2) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~sector %in% c(&quot;UK&quot;, &quot;all_dcms&quot;)) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)])) ## x &lt;- dplyr::group_by_(x, ~sector) ## x &lt;- dplyr::mutate_(x, index = ~max(ifelse(year == 2010, ## GVA, 0)), indexGVA = ~GVA/index * 100) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~indexGVA, ## x = ~year, colour = ~sector, linetype = ~sector) + ## ggplot2::geom_path(size = 1.5) + govstyle::theme_gov(base_colour = &quot;black&quot;) + ## ggplot2::scale_colour_manual(values = unname(govstyle::gov_cols[c(&quot;red&quot;, ## &quot;purple&quot;)])) + ggplot2::ylab(&quot;GVA Index: 2010=100&quot;) + ## ggplot2::theme(legend.position = &quot;bottom&quot;, legend.key = ggplot2::element_blank()) + ## ggplot2::ylim(c(80, 130)) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.2():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.2():&quot;, e) ## }, finally = { ## }) ## } body(figure3.3) ## { ## out &lt;- tryCatch(expr = { ## sectors_set &lt;- x$sectors_set ## x &lt;- dplyr::filter_(x$df, ~!sector %in% c(&quot;UK&quot;, &quot;all_dcms&quot;)) ## x$sector &lt;- factor(x = unname(sectors_set[as.character(x$sector)])) ## x &lt;- dplyr::group_by_(x, ~sector) ## x &lt;- dplyr::mutate_(x, index = ~max(ifelse(year == 2010, ## GVA, 0)), indexGVA = ~GVA/index * 100) ## p &lt;- ggplot2::ggplot(x) + ggplot2::aes_(y = ~indexGVA, ## x = ~year, colour = ~sector, linetype = ~sector) + ## ggplot2::geom_path(size = 1.5) + govstyle::theme_gov(base_colour = &quot;black&quot;) + ## ggplot2::scale_colour_brewer(palette = &quot;Set1&quot;) + ## ggplot2::ylab(&quot;GVA Index: 2010=100&quot;) + ggplot2::theme(legend.position = &quot;right&quot;, ## legend.key = ggplot2::element_blank()) + ggplot2::ylim(c(80, ## 150)) ## return(p) ## }, warning = function() { ## w &lt;- warnings() ## warning(&quot;Warning produced running figure3.2():&quot;, w) ## }, error = function(e) { ## stop(&quot;Error produced running figure3.2():&quot;, e) ## }, finally = { ## }) ## } 4.3.3.7 Error handling A point of interest in the code with which some users may be unfamiliar is tryCatch which is a function that allows the function to catch conditions such as warnings, errors and messages. We see this towards the end of the function where if either of these conditions are produced then an informative message is produced (in that it tells you in what function there was a problem). The structure here is simple and could be copied and pasted for use in automating other figures of other chapters or statistical reports. For a comprehensive introduction see Hadley’s Chapter. 4.4 Chapter plenary We have explored the eesectors package from the perspective of someone wishing to develop our own semi-automated chapter production through the development of a package in R. This package provides a useful tempplate where one could copy the foundations of the package and workflow. "],
["open.html", "Chapter 5 Open Source rather than proprietary", " Chapter 5 Open Source rather than proprietary Open source languages such as Python and R are increasing in popularity across government. One advantage of using these tools is that we can reduce the number of steps where the data needs to be moved from one program (or format) into another. This is in line with the principle of reproducibility given in guidance on producing quality analysis for government (the AQUA book), as the entire process can be represented as a single step in code, greatly reducing the likelihood of manual transcription errors. Chapter 3 discussed why spreadsheets are dangerous. Moving away from proprietary software, towards Open Source, may also have the additional benefit of being more compatable with tried and tested software development tools and techniques (as well as the obvious no longer needing to pay for it). In GDS’s project with DCMS described in Chapter 4, we decided to use the R language, however we could equally have chosen to use Python or another language entirely: the techniques we outline in this book are language agnostic. "],
["vs.html", "Chapter 6 Version Control 6.1 Introduction 6.2 Useful resources 6.3 Typical workflow", " Chapter 6 Version Control 6.1 Introduction Few software engineers would embark on a new project without using some sort of version control software. Version control software allows us to track the three Ws: Who made Which change, and Why?. Tools like git can be used to track files of any type, but are particularly useful for code in text files for example R or Python code. Whilst git can be used locally on a single machine, or many networked machines, git can also be hooked up to free cloud services such as GitHub, GitLab, or Bitbucket(https://bitbucket.org/). Each of these services provides hosting for your version control repository, and makes the code open and easy to share. The entire project we are working on with DCMS can be seen on GitHub. Obviously this won’t be appropriate for all Government projects (and solutions do exist to allow these services to be run within secure systems), but in our work with DCMS, we were able to publish all of our code openly. You can use our code to run an example based on the 2016 publication, but producing the entire publication from end to end would require access to data which is not published openly. Below is a screenshot from the commit history showing collaboration between data scientists in DCMS and GDS. The full page can be seen on GitHub. Using a service like GitHub allows us to formalise the system of quality assurance (QA) in an auditable way. We can configure GitHub to require a code review by another person before the update to the code (this is called a pull request) is accepted into the main workstream of the project. You can see this in the screenshot below which relates to a pull request which fixed a minor bug in the prototype. The work to fix it was done by a data scientist at DCMS, and reviewed by a data scientist from GDS. The open nature of the good is great for transparency and facilitates review. The entire community can contribute to helping QA your code and identify issues or bugs. If you are lucky, they will not only report the bug / issue, but may also offer a fix for your code in the form of a pull request. 6.2 Useful resources 6.2.1 Graphical user interface focus A useful book on Git and Github that should cover all your needs for those who are uncomfortable working in a command line interface. This will cover most of your Git and Github workflow needs for collaborating in a team. 6.2.2 Git and RStudio You can also use git and Github within R Studio. 6.2.3 Command line focus However, the terminal isn’t that scary really and we recommend using it from the outset. Here’s a video tutorial that provides a good introduction and does not expect any experience of using the Unix shell (the terminal or command line). For a comprehensive tome try the Pro Git book. 6.3 Typical workflow When you first start using git it can be difficult to remember all the commonly used commands (you might find it useful to keep a list of them in a text editor). We give a simple workflow here (assuming you are collaborating on Github with a small team and have set up a repo called my_repo with the origin and remote set (try to avoid hyphens in names)). Remember to remove the comments (the #) when copying and pasting into the terminal. You will also need to give your new feature branch a good name. Open your terminal (command line tool). Navigate to my_repo using cd. Check you are up-to-date: # git checkout master # git pull Create your new feature branch to work on and get to work: # git checkout -b feature/post_name Squash your commits if appropriate, then push your new branch to Github. You will want to squash all commits together associated with one discrete piece of work (e.g. coding one function). # git push origin feature/post_name -u On Github create a pull request and ask a colleague to review your changes before merging with the master branch (you can assign a reviewer in the PR page on Github). If accepted (and it passes all necessary checks) you’re new feature will have been merged on Github. Fetch these changes locally. # git checkout master # git pull You have a new master on Github. Pull it to your local machine and the development cycle starts again! CAVEAT: this workflow is not appropriate for large open collaborations, where fork and pull is preferred. "],
["package.html", "Chapter 7 Packaging Code 7.1 Essential reading", " Chapter 7 Packaging Code A package enshrines all the business knowledge used to create a corpus of work in one place; including the code and its relevant documentation. One of the difficulties that can arise in the more manual methods of statistics production is that we have many different files relating to many different stages of the process, each of which needs to be documented, and kept up to date. Part of the heavy lifting can be done here with version control as described in Chapter 6, but we can go a step further: we can create a package of code. As Hadley Wickham (author of a number of essential packages for package development) puts it for R: Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. - Hadley Wickham Since it is a matter of statute that we produce our statistical publications, it is essential that our publications are as reproducible as possible. Packaging up the code can also help with institutional knowledge transfer. This was exemplified in Chapter 4 where we explored help files associated with code using the R ? function. library(eesectors) ?clean_sic() Linking the documentation to the code makes everything much easier to understand, and can help to minimising the time taken to bring new team members up to speed. This all meets the requirements of the AQUA book in that all assumptions and constraints can be described in the package documentation asssociated tied to the relevant code. 7.1 Essential reading Hadley Wickham’s R Packages book is an excellent and comprehensive introduction to developing your own package in R. It encourages you to start with the basics and improve over time; good advice. "],
["test.html", "Chapter 8 Unit test 8.1 Further Reading", " Chapter 8 Unit test Testing is a vital part of package development. It ensures that your code does what you want it to do. We can facilitate testing and Quality Assurance (QA) by building packages with generalisable LEGO-like functions. In procedural programming, code is designed to be reused again and again with different inputs, making for simpler code that is easier to understand and audit. It means we can easily build tests to ensure the code continues to work as expected when we make changes to it. Since each function or group of functions (unit) is generic, it can be tested with a generic example, so that we know that our unit of code works as expected. If we discover cases where our units do not do perform as expected, we can codify these cases into new tests and work to fix the problem until the test passes. We may even go a step further and adopt the practice of test driven development: starting each unit of code with a test which fails, until we write code which can pass the test. Test-driven development is a useful paradigm for developing your code in a thoroughly QA’d fashion. 8.1 Further Reading Much of the heavy lifting for this kind of testing can be done in a unit testing framework, for example testthat for R, or nosetools in Python. For how to test in R read the R package, Testing Chapter "],
["travis.html", "Chapter 9 Automated Testing", " Chapter 9 Automated Testing Once we are in the habit of packaging our code and writing unit tests, we can start to use free online tools such as Travis CI, Jenkins, or Appveyor to automatically test that our package of code builds, and that the tests we have written, pass. These tools integrate with GitHub, so we can easily see when an update to the code has failed one of our tests. The green ticks in the box below show that our tests have passed for the given pull request. We can also look at our test history on Travis CI, as in the screenshot below. From this we can see that our main workstream - the default branch - has been tested 619 times to date, the last of which was one day ago, and the previous five tests have all passed without problems. "],
["code-cover.html", "Chapter 10 Code coverage", " Chapter 10 Code coverage An additional set of tools we can start to use once we begin writing our own tests is code coverage tools, for instance codecov.io, or coveralls.io. These tools are able to analyse the code we have written via hosting services like GitHub, and provide a line by line breakdown of which lines are tested, and which are not. For example, in the lines below from the file year_sector_table.R, we can see that lines 112-115 and 117-120 are not explicitly tested. In this case, we probably don’t need to worry very much, but on other occasions this might prompt us to write more tests. "],
["dep.html", "Chapter 11 Dependency management", " Chapter 11 Dependency management If you want your code to be reproducible in the long-run (i.e. so you can come back to run it next month or next year), you’ll need to track the versions of the packages that your code uses. A rigorous approach is to use packrat, http://rstudio.github.io/packrat/, which store packages in your project directory, or checkpoint, https://github.com/RevolutionAnalytics/checkpoint, which will reinstall packages available on a specified date. A quick and dirty hack is to include a chunk that runs sessionInfo() — that won’t you let easily recreate your packages as they are today, but at least you’ll know what they were. - Hadley Wickham, R for Data Science One of the problems with working with open source software is that it is quite easy to fall into a trap called ‘dependency hell’. Essentially, this occurs when the software we write depends on open source packages, which depend on other open source packages, which can depend on other packages, and on, and on. All these packages may be written by many different people, and are updated at vastly different timescales. If we fail to take account of this, then we are likely to fail at the first hurdle of reproducibility, and we may find that in a year’s time we are no longer able to reproduce the work that we previously did - or at least not without a lot of trouble. There are several ways we might get on top of this problem, but in this project we opted for using packrat, which creates a cache of all the R packages used in the project which is then version controlled on GitHub. Matt Upson has blogged about this previously in the context of writing academic works. "],
["qa-data.html", "Chapter 12 Quality Assured data 12.1 Murphy’s Law and errors in your pipeline 12.2 Error handling 12.3 Error logging", " Chapter 12 Quality Assured data All the testing we have described so far is to do with the code, and ensuring that the code does what we expect it to, but because we have written an R package, it’s also very easy for us to institute tests for the consistency of the data at the time the data is loaded. The list of tests that we might want to run is endless, and the scope of tests very much be dictated by the team which has the expert knowledge of the data. In the eesectors package we implemented two very simple checks, but these could very easily be expanded. The simplest of these is a simple test for outliers: since the data for the economic estimates is longitudinal, i.e. stretching back several years; we are able to look at the most recent values in comparison to the values from previous years. If the latest values lie within a threshold determined statistically from the other values then the data passes, if not a warning is raised. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD These kinds of automated tests are repeated every time the data are loaded, reducing the burden of QA, and the scope for human error, freeing up statistician time for identifying more subtle data quality issues which might otherwise go unnoticed. 12.1 Murphy’s Law and errors in your pipeline Paraphrasing from Advanced R by Hadley Wickham: If something can go wrong, it will: the format of the spreadsheet you normally receive your raw data in changes, the server you’re talking to may be down, your Wi-Fi drops. Any such problem may stop a piece of your pipeline (code) from doing what it is intended to do. This is not a bug; the problem did not originate from within the code itself. However, if the pipeline downstream is dependent on this code acting as intended then you have a problem, you need to deal with the error somehow. Errors aren’t caused by bugs per se, but neglecting to handle an error appropriately is a bug. 12.2 Error handling Not all problems are unexpected. For example, an input data file may be missing. We can often anticipate some of these likely problems by thinking about our users and how the code we might be implemented or misunderstood. If something goes wrong, we want the user to know about it. We can communicate to the user using a variety of conditions; messages, warnings and errors. If we want to let the user know about something fairly inoccuous or keep them informed we can use the message() function. Sometimes we might want to draw the users attention to something that might be problematic without stopping the code from running, using warning(). If there’s no way for the code to execute then a fatal error may be preferred using stop(). As an example we look to code from the RAP eesectors package produced in collaboration with DCMS. Specifically, we look at a snippet of code from the year_sector_data function. message(&#39;Checking x does not contain missing values...&#39;) if (anyNA(x)) stop(&quot;x cannot contain any missing values&quot;) message(&#39;Checking for the correct number of rows...&#39;) if (nrow(x) != length(unique(x$sector)) * length(unique(x$year))) { warning(&quot;x does not appear to be well formed. nrow(x) should equal length(unique(x$sector)) * length(unique(x$year)). Check the of x.&quot;) } message(&#39;...passed&#39;) We’ll work our way through the code above, step by step. The first line informs the user of the quality assurance that is being automatically conducted, i.e. that no data is missing (NA). The second line uses the logical if statement to assess this on x and if it were true, we use stop to produce a fatal error with a descriptive message, as below. x &lt;- c(&quot;culture&quot;, &quot;sport&quot;, NA) message(&#39;Checking x does not contain missing values...&#39;) ## Checking x does not contain missing values... if (anyNA(x)) stop(&quot;x cannot contain any missing values&quot;) ## Error in eval(expr, envir, enclos): x cannot contain any missing values The third line checks that the data has data for each sector for each year (indirectly). If not it throws up a warning, as this could be for a valid reason (e.g. a change of name of a factor level), but it’s better to let the user know rather than let it quietly pass. Thus all the expert domain knowledge can be incorporated into the code through condition handling, providing transparent quality assurance. These informative messages are useful but when used in conjuction with tryCatch, we can implement our own custom responses to a message, warning or error (this is explained in the relevant Advanced R Chapter). We demonstrate a simplified example from the eesectors package where an informative message is provided. This is achieved by wrapping the main body of the function within the tryCatch function. # Define as a method figure3.1 &lt;- function(x, ...) { out &lt;- tryCatch( expr = { p &lt;- plot(x, y) return(p) }, warning = function() { w &lt;- warnings() warning(&#39;Warning produced running figure3.1():&#39;, w) }, error = function(e) { stop(&#39;Error produced running figure3.1():&#39;, e) }, finally = {} ) } The body of a tryCatch() must be a single expression; using { we combine several expressions into a single form. This a simple addition to our function but it’s powerful in that it provides the user with more information for anticipated problems. By wrapping the code in the tryCatch function we ensure that it gets evaluated, whereas normally an error would cause the evaluation of the code to stop. Instead we get the error and the evaluation of the next line of code. try(print(this_object_does_not_exist)); print(&quot;What happens if this is not wrapped in try?&quot;) ## [1] &quot;What happens if this is not wrapped in try?&quot; 12.3 Error logging We are increasingly using R and software packages like our RAP in “operational” settings that require robust error handling and logging. In this section we describe a quick-and-dirty way to get python style multi-level log files by wrapping the futile.logger package inspired by this blog post. 12.3.1 Pipeline pitfalls Our real world scenario involves an R package that processes raw data (typically from a spreadsheet or SQL table) that is updated periodically. The raw data can come from various different sources, set up by different agencies and usually manually procured or copy and pasted together prioritising human readability over machine readability. Data could be missing or in a different layout from that which we are use to, for a variety of reasons, before it even gets to us. And then our R functions within our package may extract this data and perform various checks on it. From there a user (analyst / statistician) may put together a statistical report using Rmarkdown ultimately resulting in a html document. This pipeline has lots of steps and potential to encounter problems throughout. As we develop our RAP for our intended bespoke problem and start to use it in an operational setting, we must ensure that in this chaotic environment we protect ourselves against things going wrong without our realising. One of the methods for working with chaotic situations in operational software is to have lots and Lots and LOTS of logging. We take our inspirration from Python, which has the brilliant “logging” module that allows us to quickly set up separate output files for log statements at different levels. This is important as we have different users who may have different needs from the log files. For example, the data scientist / analyst who did the programming for the RAP package may wish to debug the code on logging an error whereas a statistician may prefer to be notified only when an ERROR or something FATAL occurred. 12.3.2 Error logging using futile.logger Fortunately there’s a package available on CRAN that makes this process easy in R called futile.logger. There are a few concepts that it’s helpful to be familiar with before proceeding which are introduced in this blog post by the package author. One approach is to replace tryCatch with the ftry function with the finally. This function integrates futile.logger with the error and warning system so problems can be caught both in the standard R warning system, while also being emitted via futile.logger. We think about how to adapt our earlier code to this function. The primary use case for futile.logger is to write out log messages. There are log writers associated with all the predefined log levels: TRACE, DEBUG, INFO, WARN, ERROR, FATAL. Log messages will only be written if the log level is equal to or more urgent than the current threshold. By default the ROOT logger is set to INFO but this can be adjusted by the user facilitating customisation of the error logging to meet the needs of the current user (by using the flog.threshold function). We demonstrate this hierarchy below by evaluating this code. # library(futile.logger) futile.logger::flog.debug(&quot;This won&#39;t print&quot;) futile.logger::flog.info(&quot;But this %s&quot;, &#39;will&#39;) futile.logger::flog.warn(&quot;As will %s&quot;, &#39;this&#39;) x &lt;- c(&quot;culture&quot;, &quot;sport&quot;, NA) message(&#39;Checking x does not contain missing values...&#39;) if (anyNA(x)) stop(&quot;x cannot contain any missing values&quot;) We start by re-writing the above code using the futile.logger high level interface. As the default setting is at the INFO log level we can use flog.trace to hide most of the checks and messages from a typical user. # Data from raw has an error x &lt;- c(&quot;culture&quot;, &quot;sport&quot;, NA) ### Non-urgent log level futile.logger::flog.trace(&quot;Checking x does not contain missing values...&quot;) ### Urgent log level, use capture to print out data structure if (anyNA(x)) { futile.logger::flog.error(&quot;x cannot contain any missing values.&quot;, x, capture = TRUE) } futile.logger::flog.info(&quot;Finished checks.&quot;) The above example can help the user identify where the pipeline is going wrong by logging the error and capturing the object x where the data is missing. This allows us to more quickly track down what’s going wrong. 12.3.3 Logging to file At the moment we default to writing our log to the console. We could write to a file if interested using the appender family of functions. # Print log messages to the console futile.logger::appender.console() # Write log messages to a file futile.logger::appender.file(&quot;rap_companion.log&quot;) # Write log messages to console and a file futile.logger::appender.tee(&quot;rap_companion.log&quot;) ======= These kinds of automated tests are repeated every time the data are loaded, reducing the burden of QA, and the scope for human error, freeing up statistician time for identifying more subtle data quality issues which might otherwise go unnoticed. &gt;&gt;&gt;&gt;&gt;&gt;&gt; 1ef3a86… Revert “Add condition handling and logging to QA chapter (#40)” (#41) "],
["pub.html", "Chapter 13 Producing the publication 13.1 Further Reading", " Chapter 13 Producing the publication R Markdown provides an unified authoring framework for data science, combining your code, its results, and your prose commentary. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more. - Hadley Wikcham, R for Data Science Everything I have talked about so far is to do with the production of the statistics themselves, not preparation of the final publication, but there are tools that can help with this too. In our project with DCMS we plan to use Rmarkdown (a flavour of markdown) to incorporate the R code into the same document as the text of the publication. Working in this way means that we can do all of the operations in a single file, so we have no problems with ensuring that our tables or figures are synced with the latest version of the text: everything can be produced in a single file. We can even produce templates with boilerplate text like: ‘this measure increased by X%’, and then automatically populate the X with the correct values when we run the code. 13.1 Further Reading You can write individual chapters using R markdown, as one file per Chapter. Alternatively you can write the whole publication using bookdown. For a basic start in bookdown try this blog post (we wrote this book using this to kick things off). "],
["final.html", "Chapter 14 Final Thoughts 14.1 User feedback 14.2 Just the beginning…", " Chapter 14 Final Thoughts It’s called Data Science for a reason, record all the data handling, experimentation and analysis in your lab-notebook. Version control is your digital lab notebook. 14.1 User feedback The RAP companion is intended to point data scientists in the Civil Service towards the Data Ops toolkit that should be used when attempting to automate some of the boring stuff for their colleagues. Your feedback is welcome and important. It will help us improve the RAP companion through further iterations. You can feedback through completing this Google form which allows us to measure your satisfaction. Or, by raising an issue on the RAP companion repo page. 14.2 Just the beginning… We’ve introduced you to the basics of reproducibility and Data Ops. For further development ideas and inspiration consider the Data Ops manifesto. "]
]
